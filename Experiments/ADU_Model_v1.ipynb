{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59a7bd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97a65bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "import deplacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c869f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "essays = pd.read_csv(\"../data/output_csv/essays.csv\")\n",
    "adus = pd.read_csv(\"../data/output_csv/adus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0288b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2doc(text):\n",
    "    return nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "497c7eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(doc=None ,mode = 'sentence'):\n",
    "    if mode=='paragraph':\n",
    "        pass\n",
    "    if mode=='sentence':\n",
    "        return [sent for sent in doc.sents] #if not sent.text.isspace()]\n",
    "    if mode =='avg_n_grams':\n",
    "        # Code to segment with 15 grams here (aveage)    \n",
    "        pass\n",
    "    if mode=='clause':\n",
    "        # Code to segment by clause\n",
    "        pass\n",
    "    if mode=='token':\n",
    "        return [token for token in doc]# if not token.isspace()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c957aeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Indexing\n",
    "def segmentation(doc=None ,mode = 'sentence'):\n",
    "    if mode=='paragraph':\n",
    "        return \n",
    "    if mode=='sentence':\n",
    "        return [(i,sent) for i,sent in enumerate(doc.sents)] #if not sent.text.isspace()]\n",
    "    if mode =='avg_n_grams':\n",
    "        # Code to segment with 15 grams here (aveage)    \n",
    "        pass\n",
    "    if mode=='clause':\n",
    "        # Code to segment by clause\n",
    "        pass\n",
    "    if mode=='token':\n",
    "        return [token for token in doc]# if not token.isspace()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dff752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conj_advs = ['moreover', 'incidentally', 'next', 'yet', 'finally', 'then', 'for example', 'thus', 'accordingly', 'namely', 'meanwhile', 'that is', 'also', 'undoubtedly', 'all in all', 'lately', 'hence', 'still', 'therefore', 'in addition', 'indeed', 'again', 'so', 'nevertheless', 'besides', 'instead', 'for instance', 'certainly', 'however', 'anyway', 'further', 'furthermore', 'similarly', 'now', 'in conclusion', 'nonetheless', 'thereafter', 'likewise', 'otherwise', 'consequently']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9c42c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_features = ['num_tokens']\n",
    "span_features = ['word_emb', 'num_tokens', 'num_verbs', 'num_pos_pronouns', 'num_conj_adv', 'num_punct']\n",
    "token_features =['word_emb']\n",
    "features_dict = dict(doc_features=doc_features, span_features=span_features, token_features=token_features)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_extensions(features_dict=None, force=True):\n",
    "    \n",
    "    # Feature Getters\n",
    "\n",
    "    def get_word_emb(obj):\n",
    "        return obj.vector\n",
    "    \n",
    "    def get_num_tokens(obj):\n",
    "        return len(obj)\n",
    "    \n",
    "    def get_num_verbs(span):\n",
    "        return sum([1 for token in span if token.pos_ == \"VERB\"])\n",
    "\n",
    "    def get_num_pos_pronouns(span):\n",
    "        return sum([1 for token in span if token.tag_ == \"PRP$\"])\n",
    "\n",
    "    def get_num_pron(span):\n",
    "        return sum([1 for token in span if token.pos_ == \"PRON\"])\n",
    "    \n",
    "    def get_num_conj_adv(span):\n",
    "        return sum([len(re.findall(adv, span.text.lower())) for adv in conj_advs])\n",
    "    \n",
    "    def get_num_punct(span):\n",
    "        return sum([1 for token in span if token.tag_ == \"PUNCT\"])\n",
    "    \n",
    "    # Set Extensions\n",
    "    \n",
    "    for feature in features_dict['doc_features']:\n",
    "        Doc.set_extension(feature, force=force, getter=locals()[f\"get_{feature}\"])\n",
    "        \n",
    "    for feature in features_dict['span_features']:\n",
    "        Span.set_extension(feature, force=force, getter=locals()[f\"get_{feature}\"])\n",
    "        \n",
    "    for feature in features_dict['token_features']:\n",
    "        Token.set_extension(feature, force=force, getter=locals()[f\"get_{feature}\"])\n",
    "        \n",
    "create_extensions(features_dict)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf9fc6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUNNING THE FUNCTIONS\n",
    "input_text = essays[essays['essay_id'] == 'essay024']['text'].iloc[0]\n",
    "doc = text2doc(input_text)\n",
    "\n",
    "units = segmentation(doc=doc, mode='sentence')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27385f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
