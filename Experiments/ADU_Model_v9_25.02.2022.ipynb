{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0350e9ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T15:26:09.633508Z",
     "start_time": "2022-02-26T15:26:07.255278Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "import re\n",
    "import benepar\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
    "\n",
    "\n",
    "\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "76e521c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T15:26:09.653984Z",
     "start_time": "2022-02-26T15:26:09.635541Z"
    }
   },
   "outputs": [],
   "source": [
    "#from spacy import displacy\n",
    "#import deplacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ac0fbbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T15:26:09.702474Z",
     "start_time": "2022-02-26T15:26:09.653984Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_features = ['num_tokens', 'para_starts']\n",
    "span_features = ['word_emb', 'num_tokens', 'num_verbs', 'num_pos_pronouns', 'num_conj_adv', 'num_punct', 'is_para_start',\n",
    "                 'index_in_doc']\n",
    "\n",
    "# getters that are not used as features\n",
    "span_utilities = ['prev_unit', 'idx_start', 'idx_end', ]\n",
    "# methods\n",
    "span_methods = ['get_nth_unit', 'get_prev_unit_attr', 'get_label_and_error', 'get_label_clpr', 'get_label']\n",
    "token_features =['word_emb']\n",
    "\n",
    "\n",
    "\n",
    "extensions_dict = dict(doc_features=doc_features, span_features=span_features+span_utilities,\n",
    "                       token_features=token_features, span_methods=span_methods)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_extensions(extensions_dict=None, force=True):\n",
    "    \n",
    "    # Features that take 'unit' as input refer to the segmentation, they do not work with just any span.\n",
    "    \n",
    "    # Property attributes\n",
    "    \n",
    "    # Store starting and ending indices of spans in the whole doc\n",
    "    # 1 list per each document: [(s1_start, s1_end), (s2_start, s2_end),.., (sn_start, sn_end)]\n",
    "    Doc.set_extension(\"units_index_list\", default=[],force=True)\n",
    "    \n",
    "    # Store essay_id within doc\n",
    "    Doc.set_extension(\"essay_id\", default=None, force=True)\n",
    "\n",
    "    \n",
    "    # Feature Getters\n",
    "    def get_label_and_error(unit, error_function='percentage_correctness'):\n",
    "        \"\"\"\n",
    "        Inputs: unit\n",
    "\n",
    "        Outputs: label for the unit and segmentation error\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        def overlap_case(unit_start, unit_end, adu_start, adu_end):\n",
    "            if adu_start >= unit_start and adu_end <= unit_end:\n",
    "                # Case 1, ADU is fully contained in UNIT\n",
    "                return 1\n",
    "\n",
    "            elif adu_start <= unit_start and adu_end <=unit_end and adu_end>=unit_start:\n",
    "\n",
    "                # Case 2, ADU starts before UNIT, start(Left) of ADU is cut\n",
    "                return 2\n",
    "\n",
    "            elif adu_start >= unit_start and adu_end >= unit_end and adu_start<unit_end:\n",
    "\n",
    "                # Case 3, ADU starts after UNIT, end(Right) of ADU is cut\n",
    "                return 3\n",
    "\n",
    "            elif adu_start < unit_start and adu_end > unit_end:\n",
    "\n",
    "                # Case 4, ADU starts before UNIT and ends after UNIT, both sides of ADU are cut\n",
    "                return 4\n",
    "\n",
    "            else: \n",
    "                # ADU does not overlap with UNIT\n",
    "                return False\n",
    "            \n",
    "\n",
    "        def percentage_correctness(unit, adu_start, adu_end, overlap_case):\n",
    "\n",
    "            if overlap_case==2:\n",
    "                adu_start = unit._.idx_start\n",
    "            elif overlap_case==3:\n",
    "                adu_end = unit._.idx_end\n",
    "            elif overlap_case==4:\n",
    "                adu_start = unit._.idx_start\n",
    "                adu_end = unit._.idx_end\n",
    "\n",
    "            adu = unit.doc.char_span(adu_start, adu_end, alignment_mode='expand')\n",
    "            \n",
    "\n",
    "            unit_ntokens = len(unit)\n",
    "            adu_ntokens = len(adu)\n",
    "            pct_correct = adu_ntokens/unit_ntokens\n",
    "            return pct_correct\n",
    "\n",
    "        def extended_accuracy(unit, adu_start, adu_end, overlap_case):\n",
    "            # Compares number of tokens to get the the correct ADU in proportional with UNIT length\n",
    "\n",
    "            if overlap_case==2:\n",
    "                adu_start = unit._.idx_start\n",
    "            if overlap_case==3:\n",
    "                adu_end = unit._.idx_end\n",
    "            adu = unit.doc.char_span(adu_start, adu_end, alignment_mode='expand')\n",
    "\n",
    "            unit_ntokens = len(unit)\n",
    "            adu_ntokens = len(adu)\n",
    "            diff_ntokens = np.abs(unit_ntokens - adu_ntokens)\n",
    "\n",
    "            return 1/((diff_ntokens+1)**(np.log2(diff_ntokens+1)/np.log2(unit_ntokens+1)))\n",
    "\n",
    "\n",
    "        if error_function.lower() == 'percentage_correctness':\n",
    "            err_func = percentage_correctness\n",
    "        elif error_function.lower() == 'extended_accuracy':\n",
    "            err_func = extended_accuracy\n",
    "        \n",
    "        unit_start = unit._.idx_start\n",
    "        unit_end = unit._.idx_end\n",
    "\n",
    "        essay_id = unit.doc._.essay_id\n",
    "\n",
    "        # DataFrame containing ADUs indices & labels, filtered for current essay_id\n",
    "        adus_doc = adus[adus['essay_id'] == essay_id]\n",
    "\n",
    "        ### WORKING 09.02.2022#$$$$$$$$$$$$\n",
    "        def segmentation_error(unit, adu_start, adu_end, overlap_case, error_function):\n",
    "            \n",
    "            adu = unit.doc.char_span(adu_start, adu_end, alignment_mode='expand')\n",
    "            \n",
    "            # positive value = too many tokens in segment, unit should be shorter (include less non-adu tokens)\n",
    "            # negative value = too less tokens in segment, unit should be longer (include more adu tokens)\n",
    "            \n",
    "            left_tokens = adu.start - unit.start\n",
    "            right_tokens = unit.end - adu.end\n",
    "            \n",
    "            if error_function.lower() == 'percentage_correctness':\n",
    "                err_func = percentage_correctness\n",
    "            elif error_function.lower() == 'extended_accuracy':\n",
    "                err_func = extended_accuracy\n",
    "\n",
    "            \n",
    "            return (left_tokens, err_func(unit, adu_start, adu_end, overlap_case), right_tokens)\n",
    "            \n",
    "# v7 returns: (ADU_Type, (left_error_tokens, err_func, right_error_tokens))\n",
    "        label_and_error = [(row['ADU_type'], segmentation_error(unit, row['start_ind'],row['end_ind'], \n",
    "                          overlap_case(unit_start, unit_end,row['start_ind'], row['end_ind']), error_function),\n",
    "                          #(row['start_ind'], row['end_ind'])\n",
    "                           ) \n",
    "                         for row_ind, row in adus_doc.iterrows() \n",
    "                        # NOT SURE ABOUT <= or < SIGNS\n",
    "                         if unit_start < row['end_ind'] and unit_end >= row['start_ind']]\n",
    "\n",
    "            \n",
    "# v6 returns: (ADU_Type, err_func)\n",
    "#\n",
    "#         label_and_error = [(row['ADU_type'], err_func(unit, row['start_ind'],row['end_ind'], \n",
    "#                           overlap_case(unit_start, unit_end,row['start_ind'], row['end_ind'])),\n",
    "#                           #(row['start_ind'], row['end_ind'])\n",
    "#                            ) \n",
    "#                          for row_ind, row in adus_doc.iterrows() \n",
    "#                          if unit_start <= row['end_ind'] and unit_end >= row['start_ind']]\n",
    "\n",
    "    #     # Contains information of the ADUs that overlap with the UNIT\n",
    "    #     # Structure: (adu_start, adu_end, overlap_case, ADU_type)\n",
    "    #     overlap_adus = [(row['start_ind'],\n",
    "    #                      row['end_ind'], \n",
    "    #                      overlap_case(unit_start, unit_end,row['start_ind'], row['end_ind']), \n",
    "    #                      row['ADU_type']) \n",
    "    #                      for row_ind, row in adus_doc.iterrows()\n",
    "    #           if unit_start <= row['end_ind'] and unit_end >= row['start_ind']]\n",
    "\n",
    "        return label_and_error\n",
    "\n",
    "    def get_label_clpr(unit, label_mode='clpr', threshold=0):\n",
    "        # DUPLICATE OF get_label\n",
    "        error_tuple = unit._.get_label_and_error()\n",
    "\n",
    "        if len(error_tuple) == 0:\n",
    "            return \"Non-ADU\"\n",
    "        else:\n",
    "            # Get position of label with maximum accuracy\n",
    "            label_position = np.argmax([error[1] for label, error in error_tuple])\n",
    "            if error_tuple[label_position][1][1] > threshold:\n",
    "                if label_mode=='clpr':\n",
    "                    label = error_tuple[label_position][0]\n",
    "                elif label_mode=='adu':\n",
    "                    label = 'ADU'\n",
    "                    \n",
    "            else:\n",
    "                label = \"Non-ADU\"\n",
    "\n",
    "            return label\n",
    "    \n",
    "    def get_label(unit, label_mode='clpr', threshold=0):\n",
    "        error_tuple = unit._.get_label_and_error()\n",
    "\n",
    "        if len(error_tuple) == 0:\n",
    "            return \"Non-ADU\"\n",
    "        else:\n",
    "            # Get position of label with maximum accuracy\n",
    "            label_position = np.argmax([error[1] for label, error in error_tuple])\n",
    "            if error_tuple[label_position][1][1] > threshold:\n",
    "                if label_mode=='clpr':\n",
    "                    label = error_tuple[label_position][0]\n",
    "                elif label_mode=='adu':\n",
    "                    label = 'ADU'\n",
    "                    \n",
    "            else:\n",
    "                label = \"Non-ADU\"\n",
    "\n",
    "            return label\n",
    "\n",
    "    def _NOT_USED_get_label_adu(span):\n",
    "        \n",
    "        # Gets ADU vs non-ADU LABEL for the span (intended only for sentences)\n",
    "\n",
    "        # Works if the span is larger or equal to the adu\n",
    "\n",
    "        # TODO:\n",
    "        # DOES NOT WORK IF SPAN IS SMALLER THAN ADU, OR IF ADU IS SPLIT BETWEEN TWO SPANS (NEEDS MORE WORK!!!)\n",
    "        # CLAIM VS PREMISE\n",
    "        essay_id = span.doc._.essay_id\n",
    "\n",
    "        span_start = span[0].idx\n",
    "        #  + len(span[-1]) to get to the end of the last word\n",
    "        span_end = span[-1].idx  + len(span[-1])\n",
    "        start_inds = adus[adus['essay_id'] == essay_id ]['start_ind'].values\n",
    "        end_inds = adus[adus['essay_id'] == essay_id ]['end_ind'].values\n",
    "\n",
    "        # Checks if starting index of span is smaller than ADU and the ending index of the span is larger than the ADU\n",
    "        return ((start_inds >= span_start) & (end_inds <= span_end)).any()\n",
    "\n",
    "    \n",
    "    def get_idx_start(unit):\n",
    "        return unit[0].idx\n",
    "    \n",
    "    def get_idx_end(unit):\n",
    "        return unit[-1].idx  + len(unit[-1])\n",
    "    \n",
    "    def get_label_pct(span):\n",
    "        \n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def get_para_starts(doc):\n",
    "        # Units starting with \\n or preceding \\n are considered as paragraph starts\n",
    "        # if start is 0, start -1 goes back to the last token of the doc\n",
    "\n",
    "        # TODO\n",
    "        # para_ends can be obtained by shifing this list to the right by one position\n",
    "        return [int(doc[start].text =='\\n' or doc[start-1].text=='\\n') for start, end in doc._.units_index_list]\n",
    "    \n",
    "    def get_is_para_start(unit):\n",
    "        \n",
    "        para_starts = unit.doc._.para_starts\n",
    "        unit_ind = unit._.index_in_doc\n",
    "        \n",
    "        return para_starts[unit_ind]\n",
    "        \n",
    "    \n",
    "    def get_word_emb(obj):\n",
    "        return obj.vector\n",
    "    \n",
    "    def get_num_tokens(obj):\n",
    "        return len(obj)\n",
    "    \n",
    "    def get_num_verbs(span):\n",
    "        return sum([1 for token in span if token.pos_ == \"VERB\"])\n",
    "\n",
    "    def get_num_pos_pronouns(span):\n",
    "        return sum([1 for token in span if token.tag_ == \"PRP$\"])\n",
    "\n",
    "    def get_num_pron(span):\n",
    "        return sum([1 for token in span if token.pos_ == \"PRON\"])\n",
    "    \n",
    "    def get_num_conj_adv(span):\n",
    "        conj_advs = ['moreover', 'incidentally', 'next', 'yet', 'finally', 'then', 'for example', 'thus', 'accordingly', 'namely', 'meanwhile', 'that is', 'also', 'undoubtedly', 'all in all', 'lately', 'hence', 'still', 'therefore', 'in addition', 'indeed', 'again', 'so', 'nevertheless', 'besides', 'instead', 'for instance', 'certainly', 'however', 'anyway', 'further', 'furthermore', 'similarly', 'now', 'in conclusion', 'nonetheless', 'thereafter', 'likewise', 'otherwise', 'consequently']\n",
    "        return sum([len(re.findall(adv, span.text.lower())) for adv in conj_advs])\n",
    "    \n",
    "    def get_num_punct(span):\n",
    "        return sum([1 for token in span if token.tag_ == \".\"])\n",
    "    \n",
    "\n",
    "    def get_index_in_doc(span):\n",
    "        \"\"\"Gets index of the segmented unit in the doc\"\"\"\n",
    "        span_start = span.start\n",
    "\n",
    "        # span end not used yet\n",
    "        span_end = span.end\n",
    "\n",
    "        # finds where span_start is in units_index_list [(s1_start, s1_end), (s2_start, s2_end),.., (sn_start, sn_end)]\n",
    "        # returns the index of the corresponding span\n",
    "        return np.where([span.start in range(start, end) for start, end in span.doc._.units_index_list])[0][-1]\n",
    "\n",
    "\n",
    "    def get_prev_unit(span):\n",
    "\n",
    "        return span._.get_nth_unit(span._.index_in_doc-1)\n",
    "    \n",
    "        \n",
    "    def get_nth_unit(span, n):\n",
    "\n",
    "        # Tuple containing the start and end index of the nth span\n",
    "        span_index = span.doc._.units_index_list[n]\n",
    "\n",
    "        # Return nth span\n",
    "        return span.doc[span_index[0]: span_index[1]]\n",
    "\n",
    "    def get_prev_unit_attr(span, attribute):\n",
    "\n",
    "        return span._.prev_unit._.get(attribute)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Iterate list of features and Set Extensions (Just to not manually set extensions one by one)\n",
    "    \n",
    "    for feature in extensions_dict['doc_features']:\n",
    "        Doc.set_extension(feature, force=force, getter=locals()[f\"get_{feature}\"])\n",
    "        \n",
    "    for feature in extensions_dict['span_features']:\n",
    "        Span.set_extension(feature, force=force, getter=locals()[f\"get_{feature}\"])\n",
    "        \n",
    "    for feature in extensions_dict['token_features']:\n",
    "        Token.set_extension(feature, force=force, getter=locals()[f\"get_{feature}\"])\n",
    "        \n",
    "    for method in extensions_dict['span_methods']:\n",
    "        Span.set_extension(method, force=force, method=locals()[method])\n",
    "\n",
    "\n",
    "def segmentation(doc=None ,mode = 'sentence', n_grams=15):\n",
    "    if mode=='paragraph':\n",
    "        pass\n",
    "    elif mode=='sentence':\n",
    "        # segment by sentences\n",
    "        units = [sent for sent in doc.sents  if not (sent.text.isspace() or sent.text =='')] \n",
    "        \n",
    "        # keep track of (start, end) of units in doc object\n",
    "        doc._.units_index_list = [(unit.start, unit.end) for unit in units]\n",
    "        return units\n",
    "    \n",
    "    elif mode =='n_grams':\n",
    "        # Code to segment with 15 grams here (average)  \n",
    "        units = [doc[i:i+n_grams] for i in range(len(doc))]\n",
    "\n",
    "        doc._.units_index_list = [(unit.start, unit.end) for unit in units]\n",
    "\n",
    "        return units\n",
    "    \n",
    "    elif mode=='clause':\n",
    "        # Code to segment by clause\n",
    "        pass\n",
    "    elif mode=='constituency1':\n",
    "        # Take the first level subordinating conjunction (SBAR)\n",
    "        # The first dependent clause\n",
    "        units = []\n",
    "        for sent in doc.sents:\n",
    "            for node in sent._.constituents:\n",
    "\n",
    "                if \"SBAR\" in node._.labels:\n",
    "\n",
    "                    # Before SBAR\n",
    "                    units.append(sent.doc[sent.start:node.start])\n",
    "                    # SBAR\n",
    "                    units.append(sent.doc[node.start:node.end])\n",
    "\n",
    "                    # After SBAR\n",
    "                    units.append(sent.doc[node.end:sent.end])\n",
    "\n",
    "                    # Break out to take only the first SBAR we encounter\n",
    "                    break\n",
    "        \n",
    "        units = [unit for unit in units if unit.text != '']\n",
    "        doc._.units_index_list = [(unit.start, unit.end) for unit in units]\n",
    "        \n",
    "        return units\n",
    "        \n",
    "    elif mode=='token':\n",
    "        return [token for token in doc if not (token.text.isspace() or token.text =='')]\n",
    "    elif mode=='gold_standard':\n",
    "        \n",
    "        # Segments ADUs according to annotations\n",
    "        \n",
    "        adu_inds = adus[adus['essay_id']==doc._.essay_id].sort_values('start_ind')[['start_ind','end_ind']]\n",
    "\n",
    "        units = []\n",
    "\n",
    "        start = 0\n",
    "        for i, row in adu_inds.iterrows():\n",
    "\n",
    "            # From previous adu end to current adu start (Non-ADU)\n",
    "            end = row['start_ind']-1\n",
    "\n",
    "            units.append(doc.char_span(start,end, alignment_mode='expand'))\n",
    "\n",
    "            start = row['start_ind']\n",
    "            end = row['end_ind']\n",
    "\n",
    "            # From current adu start to current adu end\n",
    "            units.append(doc.char_span(start,end,  alignment_mode='expand'))\n",
    "\n",
    "            # set current adu end as start for next iteration\n",
    "            start = row['end_ind']\n",
    "        \n",
    "        \n",
    "        # keep track of (start, end) of units in doc object\n",
    "        doc._.units_index_list = [(unit.start, unit.end) for unit in units]\n",
    "        \n",
    "        return units\n",
    "\n",
    "def unit2fv(unit, feature_list):\n",
    "    \n",
    "    fv = np.array([unit._.get(feature) for feature in feature_list], dtype='object')\n",
    "    \n",
    "    _fv = np.array([np.reshape(feature, -1) for feature in fv], dtype='object')\n",
    "    \n",
    "    return np.concatenate(_fv)\n",
    "\n",
    "\n",
    "def calculate_segmentation_accuracy(units, error_function='percentage_correctness'):\n",
    "    \n",
    "    \n",
    "    \n",
    "    start_errors = np.array([])\n",
    "    segmentation_accs = np.array([])\n",
    "    end_errors = np.array([])\n",
    "\n",
    "    for unit in units:\n",
    "        error_tuple = unit._.get_label_and_error(error_function=error_function)\n",
    "\n",
    "        if len(error_tuple) != 0:\n",
    "            label_position = np.argmax([error[1] for label, error in error_tuple])\n",
    "\n",
    "            start_errors = np.append(start_errors,error_tuple[label_position][1][0])\n",
    "\n",
    "            segmentation_accs = np.append(segmentation_accs, error_tuple[label_position][1][1])\n",
    "\n",
    "            end_errors = np.append(end_errors, error_tuple[label_position][1][2])\n",
    "\n",
    "\n",
    "\n",
    "    start_error = sum((start_errors**2))/len(start_errors)\n",
    "\n",
    "    end_error = sum((end_errors**2))/len(end_errors)\n",
    "\n",
    "    segmentation_acc = segmentation_accs.mean()\n",
    "    \n",
    "    return (start_error, segmentation_acc, end_error)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run\n",
    "create_extensions(extensions_dict)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8ae13757",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T15:26:09.765475Z",
     "start_time": "2022-02-26T15:26:09.704482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Claim', (0, 1.0, 0))\n",
      "311 373\n",
      "('Premise', (0, 1.0, 0))\n",
      "375 521\n",
      "('Premise', (0, 1.0, 0))\n",
      "533 617\n",
      "('Premise', (0, 1.0, 0))\n",
      "619 663\n",
      "('Claim', (0, 1.0, 0))\n",
      "694 795\n",
      "('Premise', (0, 1.0, 0))\n",
      "811 886\n",
      "('Premise', (0, 1.0, 0))\n",
      "888 973\n",
      "('Claim', (0, 1.0, 0))\n",
      "984 1093\n",
      "('Premise', (0, 1.0, 0))\n",
      "1095 1239\n",
      "('Premise', (0, 1.0, 0))\n",
      "1262 1299\n",
      "('Premise', (0, 1.0, 0))\n",
      "1308 1390\n",
      "('Claim', (0, 1.0, 0))\n",
      "1416 1478\n",
      "('Claim', (0, 1.0, 0))\n",
      "1480 1530\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADU_index</th>\n",
       "      <th>annotation</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>label</th>\n",
       "      <th>start_ind</th>\n",
       "      <th>end_ind</th>\n",
       "      <th>ADU_type_MC</th>\n",
       "      <th>ADU_text</th>\n",
       "      <th>ADU_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>T1</td>\n",
       "      <td>MajorClaim 1480 1530\\tit still has its bad sid...</td>\n",
       "      <td>essay024</td>\n",
       "      <td>train</td>\n",
       "      <td>1480</td>\n",
       "      <td>1530</td>\n",
       "      <td>MajorClaim</td>\n",
       "      <td>it still has its bad side, especially for chil...</td>\n",
       "      <td>Claim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>T2</td>\n",
       "      <td>Claim 1416 1478\\tit is undeniable that compute...</td>\n",
       "      <td>essay024</td>\n",
       "      <td>train</td>\n",
       "      <td>1416</td>\n",
       "      <td>1478</td>\n",
       "      <td>Claim</td>\n",
       "      <td>it is undeniable that computer is a crucial pa...</td>\n",
       "      <td>Claim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>T3</td>\n",
       "      <td>Claim 311 373\\tusing computer constantly has b...</td>\n",
       "      <td>essay024</td>\n",
       "      <td>train</td>\n",
       "      <td>311</td>\n",
       "      <td>373</td>\n",
       "      <td>Claim</td>\n",
       "      <td>using computer constantly has bad influence on...</td>\n",
       "      <td>Claim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>T4</td>\n",
       "      <td>Premise 375 521\\tWhen they concentrate on comp...</td>\n",
       "      <td>essay024</td>\n",
       "      <td>train</td>\n",
       "      <td>375</td>\n",
       "      <td>521</td>\n",
       "      <td>Premise</td>\n",
       "      <td>When they concentrate on computer for too long...</td>\n",
       "      <td>Premise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>T5</td>\n",
       "      <td>Premise 533 617\\tchildren who play games too m...</td>\n",
       "      <td>essay024</td>\n",
       "      <td>train</td>\n",
       "      <td>533</td>\n",
       "      <td>617</td>\n",
       "      <td>Premise</td>\n",
       "      <td>children who play games too much on computer c...</td>\n",
       "      <td>Premise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>T6</td>\n",
       "      <td>Claim 694 795\\tpeople who are addicted to game...</td>\n",
       "      <td>essay024</td>\n",
       "      <td>train</td>\n",
       "      <td>694</td>\n",
       "      <td>795</td>\n",
       "      <td>Claim</td>\n",
       "      <td>people who are addicted to games, especially o...</td>\n",
       "      <td>Claim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>T7</td>\n",
       "      <td>Premise 811 886\\tseveral teenagers play games ...</td>\n",
       "      <td>essay024</td>\n",
       "      <td>train</td>\n",
       "      <td>811</td>\n",
       "      <td>886</td>\n",
       "      <td>Premise</td>\n",
       "      <td>several teenagers play games without rest, whi...</td>\n",
       "      <td>Premise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>T8</td>\n",
       "      <td>Premise 888 973\\ta typical example is the deat...</td>\n",
       "      <td>essay024</td>\n",
       "      <td>train</td>\n",
       "      <td>888</td>\n",
       "      <td>973</td>\n",
       "      <td>Premise</td>\n",
       "      <td>a typical example is the death of Korean gamer...</td>\n",
       "      <td>Premise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>T9</td>\n",
       "      <td>Claim 984 1093\\teven people who are not intere...</td>\n",
       "      <td>essay024</td>\n",
       "      <td>train</td>\n",
       "      <td>984</td>\n",
       "      <td>1093</td>\n",
       "      <td>Claim</td>\n",
       "      <td>even people who are not interested in online g...</td>\n",
       "      <td>Claim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>T10</td>\n",
       "      <td>Premise 1095 1239\\tSome social surveys have sh...</td>\n",
       "      <td>essay024</td>\n",
       "      <td>train</td>\n",
       "      <td>1095</td>\n",
       "      <td>1239</td>\n",
       "      <td>Premise</td>\n",
       "      <td>Some social surveys have shown that a few chil...</td>\n",
       "      <td>Premise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>T11</td>\n",
       "      <td>Premise 1262 1299\\tthey will have a bad result...</td>\n",
       "      <td>essay024</td>\n",
       "      <td>train</td>\n",
       "      <td>1262</td>\n",
       "      <td>1299</td>\n",
       "      <td>Premise</td>\n",
       "      <td>they will have a bad result in school</td>\n",
       "      <td>Premise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>T12</td>\n",
       "      <td>Premise 1308 1390\\twhen they can not live with...</td>\n",
       "      <td>essay024</td>\n",
       "      <td>train</td>\n",
       "      <td>1308</td>\n",
       "      <td>1390</td>\n",
       "      <td>Premise</td>\n",
       "      <td>when they can not live without internet, they ...</td>\n",
       "      <td>Premise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>T13</td>\n",
       "      <td>Premise 619 663\\tthey will know little about t...</td>\n",
       "      <td>essay024</td>\n",
       "      <td>train</td>\n",
       "      <td>619</td>\n",
       "      <td>663</td>\n",
       "      <td>Premise</td>\n",
       "      <td>they will know little about the outside life</td>\n",
       "      <td>Premise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ADU_index                                         annotation  essay_id  \\\n",
       "338        T1  MajorClaim 1480 1530\\tit still has its bad sid...  essay024   \n",
       "339        T2  Claim 1416 1478\\tit is undeniable that compute...  essay024   \n",
       "340        T3  Claim 311 373\\tusing computer constantly has b...  essay024   \n",
       "341        T4  Premise 375 521\\tWhen they concentrate on comp...  essay024   \n",
       "342        T5  Premise 533 617\\tchildren who play games too m...  essay024   \n",
       "343        T6  Claim 694 795\\tpeople who are addicted to game...  essay024   \n",
       "344        T7  Premise 811 886\\tseveral teenagers play games ...  essay024   \n",
       "345        T8  Premise 888 973\\ta typical example is the deat...  essay024   \n",
       "346        T9  Claim 984 1093\\teven people who are not intere...  essay024   \n",
       "347       T10  Premise 1095 1239\\tSome social surveys have sh...  essay024   \n",
       "348       T11  Premise 1262 1299\\tthey will have a bad result...  essay024   \n",
       "349       T12  Premise 1308 1390\\twhen they can not live with...  essay024   \n",
       "350       T13  Premise 619 663\\tthey will know little about t...  essay024   \n",
       "\n",
       "     label  start_ind  end_ind ADU_type_MC  \\\n",
       "338  train       1480     1530  MajorClaim   \n",
       "339  train       1416     1478       Claim   \n",
       "340  train        311      373       Claim   \n",
       "341  train        375      521     Premise   \n",
       "342  train        533      617     Premise   \n",
       "343  train        694      795       Claim   \n",
       "344  train        811      886     Premise   \n",
       "345  train        888      973     Premise   \n",
       "346  train        984     1093       Claim   \n",
       "347  train       1095     1239     Premise   \n",
       "348  train       1262     1299     Premise   \n",
       "349  train       1308     1390     Premise   \n",
       "350  train        619      663     Premise   \n",
       "\n",
       "                                              ADU_text ADU_type  \n",
       "338  it still has its bad side, especially for chil...    Claim  \n",
       "339  it is undeniable that computer is a crucial pa...    Claim  \n",
       "340  using computer constantly has bad influence on...    Claim  \n",
       "341  When they concentrate on computer for too long...  Premise  \n",
       "342  children who play games too much on computer c...  Premise  \n",
       "343  people who are addicted to games, especially o...    Claim  \n",
       "344  several teenagers play games without rest, whi...  Premise  \n",
       "345  a typical example is the death of Korean gamer...  Premise  \n",
       "346  even people who are not interested in online g...    Claim  \n",
       "347  Some social surveys have shown that a few chil...  Premise  \n",
       "348              they will have a bad result in school  Premise  \n",
       "349  when they can not live without internet, they ...  Premise  \n",
       "350       they will know little about the outside life  Premise  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_errors = np.array([])\n",
    "segmentation_accs = np.array([])\n",
    "end_errors = np.array([])\n",
    "\n",
    "for unit in units:\n",
    "    error_tuple = unit._.get_label_and_error()\n",
    "\n",
    "    if len(error_tuple) != 0:\n",
    "        label_position = np.argmax([error[1] for label, error in error_tuple])\n",
    "        \n",
    "        print(error_tuple[label_position])\n",
    "        print(unit._.idx_start, unit._.idx_end)\n",
    "        start_errors = np.append(start_errors,error_tuple[label_position][1][0])\n",
    "        \n",
    "        segmentation_accs = np.append(segmentation_accs, error_tuple[label_position][1][1])\n",
    "        \n",
    "        end_errors = np.append(end_errors, error_tuple[label_position][1][2])\n",
    "        \n",
    "        \n",
    "\n",
    "start_error = sum((start_errors**2))/len(start_errors)\n",
    "\n",
    "end_error = sum((end_errors**2))/len(end_errors)\n",
    "\n",
    "segmentation_acc = segmentation_accs.mean()\n",
    "\n",
    "\n",
    "adu24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "edc555f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T15:26:09.797475Z",
     "start_time": "2022-02-26T15:26:09.767475Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62.111111111111114, 0.6772685142047236, 27.88888888888889)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "units = segmentation(doc, mode='sentence')\n",
    "calculate_segmentation_accuracy(units)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "83caef8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T15:26:12.734134Z",
     "start_time": "2022-02-26T15:26:12.691195Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "units = segmentation(doc, mode='gold_standard')\n",
    "calculate_segmentation_accuracy(units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0e39b011",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T15:26:13.648988Z",
     "start_time": "2022-02-26T15:26:13.595985Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.61111111111111, 0.8280923869159164, 82.11111111111111)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "units = segmentation(doc, mode='constituency1')\n",
    "calculate_segmentation_accuracy(units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1abd7fc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T15:26:15.301223Z",
     "start_time": "2022-02-26T15:26:15.276451Z"
    }
   },
   "outputs": [],
   "source": [
    "# Optional, not used yet. Trying to solve problem that title gets included with the first sentence\n",
    "def add_full_stops(text):\n",
    "    \"\"\"adds full stops to texts that end with \\n missing full stops\"\"\"\n",
    "    return re.sub(\"\\n+(?!\\.)\",'.\\n', text)\n",
    "# Not used\n",
    "def text2doc(text):\n",
    "    # need to use nlp.pipe here instead\n",
    "    return nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9ddebbc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T15:26:15.694189Z",
     "start_time": "2022-02-26T15:26:15.677188Z"
    }
   },
   "outputs": [],
   "source": [
    "# FOR VIEWING ACCURACY ONLY\n",
    "def all_docs(df, segmentation_mode='sentence', label_mode='adu', threshold=0, n_grams=None):\n",
    "    # Rename to create_training_data?\n",
    "    data = [(row['text'], dict(id=row['essay_id'])) for ind, row in df.iterrows()]\n",
    "    docs = []\n",
    "    data\n",
    "    for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "        doc._.essay_id = context['id']\n",
    "        docs.append(doc)\n",
    "    return docs\n",
    "    segmented_docs = [segmentation(doc, mode=segmentation_mode ,n_grams=n_grams) for doc in docs]\n",
    "    \n",
    "    # Flatten lists (Dissolve docs boundaries and store all units together in one huge list)\n",
    "    units = list(chain.from_iterable(segmented_docs))\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7adc4cc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T15:26:16.350939Z",
     "start_time": "2022-02-26T15:26:16.321979Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>essay073</td>\n",
       "      <td>Is image more powerful than the written word?\\...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>essay142</td>\n",
       "      <td>Improved medical care\\n\\nAlong with the develo...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>essay398</td>\n",
       "      <td>We can not forcedly put the same numbers of ma...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>essay385</td>\n",
       "      <td>Do guns can really increase the level of viole...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>essay278</td>\n",
       "      <td>Financial support for sports and social activi...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>essay023</td>\n",
       "      <td>Effects of mobile phones\\n\\nNowadays, the popu...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>essay115</td>\n",
       "      <td>Advanced fertilizers and machine for farmers, ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>essay399</td>\n",
       "      <td>Drugs, alcohol and messy sex lives\\n\\nCelebrit...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>essay263</td>\n",
       "      <td>Whether it is better to have broad knowledge o...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>essay209</td>\n",
       "      <td>University education restriction\\n\\nUniverstiy...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>essay057</td>\n",
       "      <td>Creativity - should it be given freedom or res...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>essay094</td>\n",
       "      <td>Prevention is better than cure\\n\\nIt is a comp...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>essay401</td>\n",
       "      <td>Fatherhood should be as present as motherhood ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>essay226</td>\n",
       "      <td>Success and knowledge\\n\\nMany people think tha...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>essay212</td>\n",
       "      <td>Follow the New Customs of the New Country or N...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>essay392</td>\n",
       "      <td>The environment problems facing today's world ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>essay056</td>\n",
       "      <td>Influence of English - advantages outweigh the...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>essay149</td>\n",
       "      <td>The opportunity to receive education from univ...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>essay182</td>\n",
       "      <td>Should the Government Provide Free College?\\n\\...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>essay043</td>\n",
       "      <td>Sporting events easing international tensions,...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>essay127</td>\n",
       "      <td>People are put too much focus on making wealth...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>essay256</td>\n",
       "      <td>University students should attend classes or n...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>essay001</td>\n",
       "      <td>Should students be taught to compete or to coo...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>essay334</td>\n",
       "      <td>Advantages and disadvantages of machines inste...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>essay195</td>\n",
       "      <td>A university plans to devlope a new research c...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>essay286</td>\n",
       "      <td>Experiences that have shaped your outlook and ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>essay083</td>\n",
       "      <td>The nuclear power provide clean and cheap ener...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>essay323</td>\n",
       "      <td>Parents are our teachers from the very early t...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>essay058</td>\n",
       "      <td>Competition or co-operation-which is better\\n\\...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>essay085</td>\n",
       "      <td>Use of CCTV cameras\\n\\nToday, close circuit te...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>essay105</td>\n",
       "      <td>Should the cost of medicines be reduced?\\n\\nAt...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>essay363</td>\n",
       "      <td>Machines - positive or negative for humans\\n\\n...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>essay114</td>\n",
       "      <td>There will be soon no role for teachers in cla...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>essay282</td>\n",
       "      <td>Best ways of reducing stress: listening to mus...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>essay016</td>\n",
       "      <td>Using animals for the benefit of the human bei...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>essay034</td>\n",
       "      <td>Study at school or get a job?\\n\\nMany people b...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>essay026</td>\n",
       "      <td>Prepared Food\\n\\nNowadays, more and more peopl...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>essay117</td>\n",
       "      <td>Can technology alone solve the world's environ...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>essay350</td>\n",
       "      <td>Internet will end the era of newspapers and ma...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>essay335</td>\n",
       "      <td>There have been significant developments in th...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>essay010</td>\n",
       "      <td>Should governments spend more money on improvi...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>essay394</td>\n",
       "      <td>Is it necessary to teach children handwriting?...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>essay233</td>\n",
       "      <td>Easy preparation of food leads to a better lif...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>essay174</td>\n",
       "      <td>Serious or entertaining movies\\n\\nI prefer the...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>essay078</td>\n",
       "      <td>Economic development vs environment\\n\\nCurrent...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>essay095</td>\n",
       "      <td>The popularity of news media\\n\\nNowadays news ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>essay077</td>\n",
       "      <td>Zoos should be banned\\n\\nChildren tend to visi...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>essay133</td>\n",
       "      <td>Nowadays human activities are influenced by co...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>essay382</td>\n",
       "      <td>Technology helps student learn more informatio...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>essay159</td>\n",
       "      <td>Popularity of mobile phones to young people\\n\\...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     essay_id                                               text  label\n",
       "72   essay073  Is image more powerful than the written word?\\...  train\n",
       "141  essay142  Improved medical care\\n\\nAlong with the develo...   test\n",
       "397  essay398  We can not forcedly put the same numbers of ma...   test\n",
       "384  essay385  Do guns can really increase the level of viole...  train\n",
       "277  essay278  Financial support for sports and social activi...   test\n",
       "22   essay023  Effects of mobile phones\\n\\nNowadays, the popu...  train\n",
       "114  essay115  Advanced fertilizers and machine for farmers, ...  train\n",
       "398  essay399  Drugs, alcohol and messy sex lives\\n\\nCelebrit...  train\n",
       "262  essay263  Whether it is better to have broad knowledge o...  train\n",
       "208  essay209  University education restriction\\n\\nUniverstiy...  train\n",
       "56   essay057  Creativity - should it be given freedom or res...  train\n",
       "93   essay094  Prevention is better than cure\\n\\nIt is a comp...  train\n",
       "400  essay401  Fatherhood should be as present as motherhood ...  train\n",
       "225  essay226  Success and knowledge\\n\\nMany people think tha...  train\n",
       "211  essay212  Follow the New Customs of the New Country or N...   test\n",
       "391  essay392  The environment problems facing today's world ...  train\n",
       "55   essay056  Influence of English - advantages outweigh the...  train\n",
       "148  essay149  The opportunity to receive education from univ...   test\n",
       "181  essay182  Should the Government Provide Free College?\\n\\...   test\n",
       "42   essay043  Sporting events easing international tensions,...  train\n",
       "126  essay127  People are put too much focus on making wealth...  train\n",
       "255  essay256  University students should attend classes or n...  train\n",
       "0    essay001  Should students be taught to compete or to coo...  train\n",
       "333  essay334  Advantages and disadvantages of machines inste...  train\n",
       "194  essay195  A university plans to devlope a new research c...  train\n",
       "285  essay286  Experiences that have shaped your outlook and ...  train\n",
       "82   essay083  The nuclear power provide clean and cheap ener...  train\n",
       "322  essay323  Parents are our teachers from the very early t...  train\n",
       "57   essay058  Competition or co-operation-which is better\\n\\...  train\n",
       "84   essay085  Use of CCTV cameras\\n\\nToday, close circuit te...  train\n",
       "104  essay105  Should the cost of medicines be reduced?\\n\\nAt...  train\n",
       "362  essay363  Machines - positive or negative for humans\\n\\n...  train\n",
       "113  essay114  There will be soon no role for teachers in cla...  train\n",
       "281  essay282  Best ways of reducing stress: listening to mus...  train\n",
       "15   essay016  Using animals for the benefit of the human bei...  train\n",
       "33   essay034  Study at school or get a job?\\n\\nMany people b...  train\n",
       "25   essay026  Prepared Food\\n\\nNowadays, more and more peopl...  train\n",
       "116  essay117  Can technology alone solve the world's environ...   test\n",
       "349  essay350  Internet will end the era of newspapers and ma...  train\n",
       "334  essay335  There have been significant developments in th...   test\n",
       "9    essay010  Should governments spend more money on improvi...  train\n",
       "393  essay394  Is it necessary to teach children handwriting?...  train\n",
       "232  essay233  Easy preparation of food leads to a better lif...  train\n",
       "173  essay174  Serious or entertaining movies\\n\\nI prefer the...  train\n",
       "77   essay078  Economic development vs environment\\n\\nCurrent...  train\n",
       "94   essay095  The popularity of news media\\n\\nNowadays news ...  train\n",
       "76   essay077  Zoos should be banned\\n\\nChildren tend to visi...   test\n",
       "132  essay133  Nowadays human activities are influenced by co...  train\n",
       "381  essay382  Technology helps student learn more informatio...   test\n",
       "158  essay159  Popularity of mobile phones to young people\\n\\...  train"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essays = essays.sample(n=50, random_state=42)\n",
    "\n",
    "essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "137d6c2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T15:27:03.608118Z",
     "start_time": "2022-02-26T15:27:03.556807Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.61111111111111, 0.8280923869159164, 82.11111111111111)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_segmentation_accuracy(units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6f84ba1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T15:26:47.749813Z",
     "start_time": "2022-02-26T15:26:16.770726Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BilalMehyar\\anaconda3\\lib\\site-packages\\torch\\distributions\\distribution.py:44: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\BILALM~1\\AppData\\Local\\Temp/ipykernel_20928/3287958.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_docs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0messays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\BILALM~1\\AppData\\Local\\Temp/ipykernel_20928/1361873243.py\u001b[0m in \u001b[0;36mall_docs\u001b[1;34m(df, segmentation_mode, label_mode, threshold, n_grams)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_tuples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0messay_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mdocs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mpipe\u001b[1;34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[0m\n\u001b[0;32m   1532\u001b[0m                 \u001b[0mcomponent_cfg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1533\u001b[0m             )\n\u001b[1;32m-> 1534\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1535\u001b[0m                 \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1536\u001b[0m                 \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mpipe\u001b[1;34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[0m\n\u001b[0;32m   1576\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mpipe\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpipes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1577\u001b[0m                 \u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1578\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1579\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36m_pipe\u001b[1;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1610\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1611\u001b[1;33m                 \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1612\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\benepar\\integrations\\spacy_plugin.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    149\u001b[0m         for sent, parse in zip(\n\u001b[0;32m    150\u001b[0m             \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m             self._parser.parse(\n\u001b[0m\u001b[0;32m    152\u001b[0m                 \u001b[0mwrapped_sents\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m                 \u001b[0mreturn_compressed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\benepar\\parse_chart.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, examples, return_compressed, return_scores, subbatch_max_tokens)\u001b[0m\n\u001b[0;32m    414\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msubbatch_max_tokens\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m             res = subbatching.map(\n\u001b[0m\u001b[0;32m    417\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_encoded\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m                 \u001b[0mexamples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\benepar\\subbatching.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(func, costs, max_cost, *data, **common_kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mitem_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0msubbatch_items\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcosts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcosts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_cost\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_cost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0msubbatch_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msubbatch_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcommon_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mitem_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem_out\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubbatch_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m             \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\benepar\\parse_chart.py\u001b[0m in \u001b[0;36m_parse_encoded\u001b[1;34m(self, examples, encoded, return_compressed, return_scores)\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_encoded\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m             \u001b[0mspan_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreturn_scores\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m                 \u001b[0mspan_scores_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspan_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\benepar\\parse_chart.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    282\u001b[0m                 ].to(self.device)\n\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m             pretrained_out = self.pretrained_model(\n\u001b[0m\u001b[0;32m    285\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpretrained_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1366\u001b[0m         \u001b[1;31m# Encode if needed (training, first prediction pass)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1368\u001b[1;33m             encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m   1369\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1370\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1001\u001b[0m                 )\n\u001b[0;32m   1002\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1003\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m   1004\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m         \u001b[1;31m# Apply Feed Forward layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 691\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m         \u001b[1;31m# clamp inf values to enable fp16 training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[0mforwarded_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m         \u001b[0mforwarded_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDenseReluDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "docs = all_docs(essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4d36aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T15:26:47.751816Z",
     "start_time": "2022-02-26T15:26:47.751816Z"
    }
   },
   "outputs": [],
   "source": [
    "#units_sentence = all_units(essays, segmentation_mode='sentence')\n",
    "units_const = all_units(essays, segmentation_mode='constituency1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44404e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T15:26:47.753826Z",
     "start_time": "2022-02-26T15:26:47.753826Z"
    }
   },
   "outputs": [],
   "source": [
    "calculate_segmentation_accuracy(units_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010a19ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T15:26:47.754817Z",
     "start_time": "2022-02-26T15:26:47.754817Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pipelinev1\n",
    "\n",
    "def text2fv(df, segmentation_mode='sentence', label_mode='adu', threshold=0, n_grams=None):\n",
    "    # Rename to create_training_data?\n",
    "    data = [(row['text'], dict(id=row['essay_id'])) for ind, row in df.iterrows()]\n",
    "    docs = []\n",
    "    data\n",
    "    for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "        doc._.essay_id = context['id']\n",
    "        docs.append(doc)\n",
    "        \n",
    "    segmented_docs = [segmentation(doc, mode=segmentation_mode ,n_grams=n_grams) for doc in docs]\n",
    "    \n",
    "    # Flatten lists (Dissolve docs boundaries and store all units together in one huge list)\n",
    "    units = list(chain.from_iterable(segmented_docs))\n",
    "\n",
    "    X_features = span_features\n",
    "    \n",
    "\n",
    "    X = np.array([unit2fv(unit, X_features) for unit in units])\n",
    "    y = np.array([unit._.get_label(label_mode=label_mode, threshold=threshold) for unit in units])\n",
    "\n",
    "    return X,y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1473cad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T15:26:47.756816Z",
     "start_time": "2022-02-26T15:26:47.756816Z"
    }
   },
   "outputs": [],
   "source": [
    "# INPUTS \n",
    "essays = pd.read_csv(\"../data/output_csv/essays.csv\")\n",
    "adus = pd.read_csv(\"../data/output_csv/adus.csv\")\n",
    "\n",
    "###### TEST\n",
    "in_text = essays.iloc[23].text\n",
    "doc = nlp(in_text)\n",
    "doc._.essay_id = essays.iloc[23]['essay_id']\n",
    "adu24 = adus[adus['essay_id'] == doc._.essay_id]\n",
    "units=segmentation(doc, mode='n_grams', n_grams=15)\n",
    "units=segmentation(doc, mode='sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a178014",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T15:26:47.757816Z",
     "start_time": "2022-02-26T15:26:47.757816Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility, Delete Later\n",
    "def print_adus(units):\n",
    "    for i, u in enumerate(units):\n",
    "            unit_start = u._.idx_start\n",
    "            unit_end = u._.idx_end\n",
    "            \n",
    "            essay_id = u.doc._.essay_id\n",
    "            \n",
    "            \n",
    "            doc_adus = adus[adus['essay_id'] == essay_id]\n",
    "             \n",
    "            lis = [((unit_start, unit_end),(row['start_ind'], row['end_ind'], row['ADU_type'],is_adu(unit_start, unit_end, row['start_ind'] ,row['end_ind']))) for row_ind,row in doc_adus.iterrows() if is_adu(unit_start, unit_end, row['start_ind'] ,row['end_ind'])]\n",
    "            \n",
    "            \n",
    "            \n",
    "            if len(lis)>0:\n",
    "                print(i, lis)\n",
    "\n",
    "def verbose_print(units):\n",
    "    # Detailed Printer\n",
    "    \n",
    "    essay_id = units[0].doc._.essay_id\n",
    "    adu_doc = adus[adus['essay_id']==essay_id]\n",
    "    for i, u in enumerate(units):\n",
    "            span_start = u[0].idx\n",
    "            span_end = u[-1].idx  + len(u[-1])\n",
    "\n",
    "            lis = [((span_start, span_end),(row['start_ind'], row['end_ind'], row['ADU_type'],\n",
    "                                            is_adu(span_start, span_end, row['start_ind'] ,row['end_ind'])))\n",
    "                   for row_ind,row in adu_doc.iterrows() if is_adu(span_start, span_end, row['start_ind'] ,row['end_ind'])]\n",
    "            if len(lis)>0:\n",
    "                print(i)\n",
    "                print(lis,\"\\n\")\n",
    "                print(\"UNIT:\",u,\"\\n\")\n",
    "                for ind, adu in enumerate(lis):\n",
    "\n",
    "                    #print(adu[1][2].upper()+':',adu[1][0:2])\n",
    "                    #print(doc.char_span(*adu[1][0:2]), \"\\n\")\n",
    "                    label = adu[1][2].upper() \n",
    "                    adu_range = adu[1][0:2]\n",
    "                    adu_status = adu[1][3]\n",
    "                    print(f'ADU #{ind+1}',label+':',*adu_range, adu_status)\n",
    "                    print(doc.char_span(*adu_range), \"\\n\")\n",
    "                print(\"-----------------\\n\")\n",
    "                \n",
    "def is_adu(unit_start, unit_end, adu_start, adu_end):\n",
    "    \n",
    "    if adu_start<=unit_start and adu_end <=unit_start:\n",
    "        # ADU comes before UNIT\n",
    "        return False\n",
    "    elif adu_start>=unit_end and adu_end >=unit_end:\n",
    "        # ADU comes after UNIT\n",
    "        return False\n",
    "    else:\n",
    "        if adu_start >= unit_start and adu_end <= unit_end:\n",
    "            #print(\"Fully Contains ADU\")\n",
    "            return \"Full\"\n",
    "        elif adu_start <= unit_start and adu_end <=unit_end:\n",
    "            \n",
    "            #print(\"ADU start is cut\")\n",
    "            return \"Start_Cut\"\n",
    "        elif adu_start >= unit_start and adu_end >= unit_end:\n",
    "            # End of ADU is after UNIT\n",
    "            return \"End_Cut\"\n",
    "\n",
    "        elif adu_start <= unit_start and adu_end >= unit_end:\n",
    "\n",
    "            # UNIT is smaller than ADU, ADU start and end are cut\n",
    "            return \"Both_Sides_Cut\"# Utility, Delete Later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b079f343",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T15:26:47.759818Z",
     "start_time": "2022-02-26T15:26:47.759818Z"
    }
   },
   "outputs": [],
   "source": [
    "units = segmentation(doc, mode='gold_standard')\n",
    "# Coding Error Evaluation\n",
    "start_errors = np.array([])\n",
    "segmentation_accs = np.array([])\n",
    "end_errors = np.array([])\n",
    "\n",
    "for unit in units:\n",
    "    error_tuple = unit._.get_label_and_error()\n",
    "\n",
    "    if len(error_tuple) != 0:\n",
    "        label_position = np.argmax([error[1] for label, error in error_tuple])\n",
    "        \n",
    "        print(error_tuple[label_position])\n",
    "        start_errors = np.append(start_errors,error_tuple[label_position][1][0])\n",
    "        \n",
    "        segmentation_accs = np.append(segmentation_accs, error_tuple[label_position][1][1])\n",
    "        \n",
    "        end_errors = np.append(end_errors, error_tuple[label_position][1][2])\n",
    "        \n",
    "        \n",
    "\n",
    "start_error = sum((start_errors**2))/len(start_errors)\n",
    "\n",
    "end_error = sum((end_errors**2))/len(end_errors)\n",
    "\n",
    "segmentation_acc = segmentation_accs.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6509b611",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T15:26:47.760817Z",
     "start_time": "2022-02-26T15:26:47.760817Z"
    }
   },
   "outputs": [],
   "source": [
    "# Coding constituency\n",
    "doc\n",
    "units = []\n",
    "\n",
    "for sent in doc.sents:\n",
    "    for node in sent._.constituents:\n",
    "\n",
    "        if \"SBAR\" in node._.labels:\n",
    "            \n",
    "            # Before SBAR\n",
    "            units.append(sent.doc[sent.start:node.start])\n",
    "            # SBAR\n",
    "            units.append(sent.doc[node.start:node.end])\n",
    "\n",
    "            # After SBAR\n",
    "            units.append(sent.doc[node.end:sent.end])\n",
    "            \n",
    "            # Break out to take only the first SBAR we encounter\n",
    "            break\n",
    "        \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2d4c61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T15:26:47.762816Z",
     "start_time": "2022-02-26T15:26:47.762816Z"
    }
   },
   "outputs": [],
   "source": [
    "# Smaller set\n",
    "essays= essays[:30].copy()\n",
    "\n",
    "train = essays[essays['label'] =='train']\n",
    "test =essays[essays['label'] =='test']\n",
    "\n",
    "X_train, y_train = text2fv(train)\n",
    "\n",
    "X_test, y_test = text2fv(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9167b6",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22c2af60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T13:08:38.285347Z",
     "start_time": "2022-02-23T13:08:38.285347Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "171eac00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T13:08:38.288348Z",
     "start_time": "2022-02-23T13:08:38.288348Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='newton-cg')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver='newton-cg')\n",
    "logreg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96892454",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T13:08:38.290351Z",
     "start_time": "2022-02-23T13:08:38.290351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADU       0.91      0.90      0.90        58\n",
      "     Non-ADU       0.25      0.29      0.27         7\n",
      "\n",
      "    accuracy                           0.83        65\n",
      "   macro avg       0.58      0.59      0.59        65\n",
      "weighted avg       0.84      0.83      0.84        65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds_lr = logreg.predict(X_test)\n",
    "print(classification_report(y_test, preds_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5da68e",
   "metadata": {},
   "source": [
    "# Two Binary Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af54034c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T13:08:38.293349Z",
     "start_time": "2022-02-23T13:08:38.293349Z"
    }
   },
   "outputs": [],
   "source": [
    "# Smaller set + Cl1\n",
    "essays= essays[:30].copy()\n",
    "\n",
    "train = essays[essays['label'] =='train']\n",
    "test =essays[essays['label'] =='test']\n",
    "\n",
    "X_train, y_train = text2fv(train, segmentation_mode='sentence', label_mode='adu')\n",
    "\n",
    "X_test, y_test = text2fv(train, segmentation_mode='sentence', label_mode='adu')\n",
    "\n",
    "\n",
    "X_train_clpr, y_train_clpr = text2fv(train, segmentation_mode='sentence', label_mode='clpr')\n",
    "\n",
    "X_test_clpr, y_test_clpr = text2fv(train, segmentation_mode='sentence', label_mode='clpr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d06ba04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T13:08:38.295347Z",
     "start_time": "2022-02-23T13:08:38.295347Z"
    }
   },
   "outputs": [],
   "source": [
    "clpr_index_train = np.where(y_train_clpr!='Non-ADU')[0]\n",
    "\n",
    "clpr_index_test = np.where(y_test_clpr!='Non-ADU')[0]\n",
    "\n",
    "X_train_clpr_only = X_train[clpr_index_train].copy()\n",
    "X_test_clpr_only = X_test[clpr_index_test].copy()\n",
    "\n",
    "\n",
    "y_train_clpr_only = y_train_clpr[clpr_index_train].copy()\n",
    "\n",
    "y_test_clpr_only = y_test_clpr[clpr_index_test].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a16985b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T10:40:54.462464Z",
     "start_time": "2022-02-11T10:40:54.416459Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='newton-cg')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl1 = LogisticRegression(solver='newton-cg')\n",
    "cl1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1b419af2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T10:40:54.896000Z",
     "start_time": "2022-02-11T10:40:54.864998Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='newton-cg')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl2 = LogisticRegression(solver='newton-cg')\n",
    "cl2.fit(X_train_clpr_only, y_train_clpr_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "02ab3bea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T10:49:08.765274Z",
     "start_time": "2022-02-11T10:49:08.752277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Claim       0.58      0.68      0.63       111\n",
      "     Non-ADU       0.59      0.90      0.71        67\n",
      "     Premise       0.85      0.67      0.75       256\n",
      "\n",
      "    accuracy                           0.71       434\n",
      "   macro avg       0.67      0.75      0.70       434\n",
      "weighted avg       0.74      0.71      0.71       434\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds_cl1 = cl1.predict(X_test)\n",
    "\n",
    "preds_cl1_adu_index = np.where(preds_cl1=='ADU')\n",
    "\n",
    "\n",
    "X_test_cl1_pred_adu = X_test[preds_cl1_adu_index]\n",
    "y_test_cl1_pred_adu = y_test[preds_cl1_adu_index]\n",
    "\n",
    "\n",
    "preds_cl2 = cl2.predict(X_test_cl1_pred_adu)\n",
    "\n",
    "preds_all = preds_cl1.copy()\n",
    "preds_all[preds_cl1_adu_index] = preds_cl2\n",
    "preds_all \n",
    "\n",
    "print(classification_report(preds_all, y_test_clpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1cba23df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T10:47:30.078505Z",
     "start_time": "2022-02-11T10:47:30.071503Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Premise', 'Claim', 'Premise', 'Premise', 'Premise', 'Premise',\n",
       "       'Claim', 'Claim', 'Claim', 'Premise', 'Premise', 'Premise',\n",
       "       'Claim', 'Claim', 'Claim', 'Claim', 'Premise', 'Premise',\n",
       "       'Premise', 'Claim', 'Claim', 'Premise', 'Premise', 'Premise',\n",
       "       'Premise', 'Claim', 'Claim', 'Premise', 'Claim', 'Claim',\n",
       "       'Premise', 'Premise', 'Claim', 'Claim', 'Premise', 'Premise',\n",
       "       'Premise', 'Claim', 'Premise', 'Claim', 'Premise', 'Premise',\n",
       "       'Premise', 'Premise', 'Claim', 'Premise', 'Premise', 'Claim',\n",
       "       'Premise', 'Claim', 'Premise', 'Premise', 'Claim', 'Premise',\n",
       "       'Premise', 'Premise', 'Claim', 'Premise', 'Premise', 'Claim',\n",
       "       'Premise', 'Premise', 'Premise', 'Claim', 'Premise', 'Premise',\n",
       "       'Premise', 'Premise', 'Premise', 'Premise', 'Premise', 'Premise',\n",
       "       'Premise', 'Premise', 'Premise', 'Premise', 'Claim', 'Premise',\n",
       "       'Premise', 'Premise', 'Premise', 'Claim', 'Premise', 'Claim',\n",
       "       'Premise', 'Premise', 'Premise', 'Premise', 'Claim', 'Premise',\n",
       "       'Premise', 'Premise', 'Premise', 'Premise', 'Claim', 'Claim',\n",
       "       'Premise', 'Claim', 'Claim', 'Premise', 'Premise', 'Premise',\n",
       "       'Premise', 'Premise', 'Premise', 'Premise', 'Premise', 'Premise',\n",
       "       'Premise', 'Claim', 'Premise', 'Premise', 'Premise', 'Claim',\n",
       "       'Premise', 'Claim', 'Premise', 'Premise', 'Premise', 'Premise',\n",
       "       'Claim', 'Premise', 'Premise', 'Premise', 'Claim', 'Premise',\n",
       "       'Premise', 'Premise', 'Premise', 'Claim', 'Premise', 'Premise',\n",
       "       'Claim', 'Premise', 'Premise', 'Premise', 'Premise', 'Premise',\n",
       "       'Premise', 'Premise', 'Premise', 'Premise', 'Premise', 'Premise',\n",
       "       'Premise', 'Claim', 'Premise', 'Premise', 'Premise', 'Premise',\n",
       "       'Premise', 'Premise', 'Claim', 'Premise', 'Premise', 'Premise',\n",
       "       'Premise', 'Claim', 'Premise', 'Premise', 'Claim', 'Premise',\n",
       "       'Premise', 'Claim', 'Claim', 'Premise', 'Premise', 'Premise',\n",
       "       'Premise', 'Premise', 'Premise', 'Premise', 'Premise', 'Premise',\n",
       "       'Premise', 'Premise', 'Premise', 'Premise', 'Premise', 'Premise',\n",
       "       'Claim', 'Premise', 'Premise', 'Claim', 'Premise', 'Premise',\n",
       "       'Claim', 'Claim', 'Premise', 'Premise', 'Claim', 'Claim',\n",
       "       'Premise', 'Premise', 'Claim', 'Claim', 'Premise', 'Premise',\n",
       "       'Premise', 'Claim', 'Premise', 'Premise', 'Claim', 'Claim',\n",
       "       'Premise', 'Premise', 'Premise', 'Claim', 'Claim', 'Premise',\n",
       "       'Premise', 'Claim', 'Premise', 'Premise', 'Premise', 'Claim',\n",
       "       'Premise', 'Claim', 'Premise', 'Premise', 'Premise', 'Premise',\n",
       "       'Premise', 'Premise', 'Claim', 'Claim', 'Premise', 'Claim',\n",
       "       'Claim', 'Premise', 'Premise', 'Premise', 'Premise', 'Premise',\n",
       "       'Premise', 'Premise', 'Premise', 'Claim', 'Premise', 'Premise',\n",
       "       'Premise', 'Premise', 'Claim', 'Premise', 'Premise', 'Claim',\n",
       "       'Premise', 'Premise', 'Premise', 'Claim', 'Premise', 'Premise',\n",
       "       'Premise', 'Premise', 'Claim', 'Claim', 'Claim', 'Claim',\n",
       "       'Premise', 'Claim', 'Premise', 'Premise', 'Premise', 'Claim',\n",
       "       'Premise', 'Premise', 'Claim', 'Claim', 'Claim', 'Premise',\n",
       "       'Premise', 'Premise', 'Claim', 'Premise', 'Premise', 'Premise',\n",
       "       'Premise', 'Premise', 'Premise', 'Premise', 'Premise', 'Premise',\n",
       "       'Premise', 'Premise', 'Claim', 'Premise', 'Premise', 'Claim',\n",
       "       'Premise', 'Premise', 'Claim', 'Premise', 'Premise', 'Premise',\n",
       "       'Premise', 'Premise', 'Premise', 'Claim', 'Premise', 'Premise',\n",
       "       'Claim', 'Premise', 'Premise', 'Premise', 'Premise', 'Premise',\n",
       "       'Claim', 'Premise', 'Premise', 'Premise', 'Premise', 'Claim',\n",
       "       'Premise', 'Premise', 'Premise', 'Premise', 'Premise', 'Premise',\n",
       "       'Claim', 'Premise', 'Claim', 'Claim', 'Premise', 'Premise',\n",
       "       'Claim', 'Premise', 'Premise', 'Premise', 'Premise', 'Claim',\n",
       "       'Claim', 'Claim', 'Claim', 'Claim', 'Claim', 'Premise', 'Premise',\n",
       "       'Claim', 'Premise', 'Claim', 'Premise', 'Premise', 'Premise',\n",
       "       'Claim', 'Claim', 'Claim', 'Premise', 'Claim', 'Premise',\n",
       "       'Premise', 'Premise', 'Claim', 'Premise', 'Premise', 'Premise',\n",
       "       'Claim', 'Premise', 'Claim', 'Claim', 'Claim', 'Premise',\n",
       "       'Premise', 'Claim', 'Premise', 'Premise', 'Claim', 'Claim'],\n",
       "      dtype='<U7')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7896d16f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T21:58:16.895685Z",
     "start_time": "2022-01-20T21:58:16.835682Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6ca98b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a111222f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T21:58:17.050698Z",
     "start_time": "2022-01-20T21:58:16.896688Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da87119a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T21:58:17.074700Z",
     "start_time": "2022-01-20T21:58:17.052702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 168,   94],\n",
       "       [  22, 1113]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = rf.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b00d11f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.64      0.74       262\n",
      "           1       0.92      0.98      0.95      1135\n",
      "\n",
      "    accuracy                           0.92      1397\n",
      "   macro avg       0.90      0.81      0.85      1397\n",
      "weighted avg       0.92      0.92      0.91      1397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d76c42f",
   "metadata": {},
   "source": [
    "## CrossValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7696698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "203bf524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(random_state=42)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Stochastic Gradient Descent (SGD) classifier, \n",
    "This classifier has the advantage of being capable of handling very large datasets efficiently\"\"\"\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f9ae099f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7224231464737794\n",
      "[[181  53]\n",
      " [254 618]]\n",
      "0.8707052441229657\n",
      "[[145  89]\n",
      " [ 54 818]]\n",
      "0.8471971066907775\n",
      "[[110 124]\n",
      " [ 45 827]]\n",
      "0.8426763110307414\n",
      "[[ 69 164]\n",
      " [ 10 863]]\n",
      "0.8090497737556561\n",
      "[[183  50]\n",
      " [161 711]]\n"
     ]
    }
   ],
   "source": [
    "skfolds = StratifiedKFold(n_splits=5, random_state=42)\n",
    "\n",
    "best_model = None \n",
    "precision = 0\n",
    "for train_index, test_index in skfolds.split(X_train, y_train):\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    X_train_folds = X_train[train_index]\n",
    "    y_train_folds = y_train[train_index]\n",
    "    X_test_fold = X_train[test_index]\n",
    "    y_test_fold = y_train[test_index]\n",
    "    \n",
    "    clone_clf.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    if precision < n_correct / len(y_pred):\n",
    "        best_model = clone_clf\n",
    "        precision = n_correct / len(y_pred)\n",
    "    print(n_correct / len(y_pred))\n",
    "    print(confusion_matrix(y_test_fold, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "379c52c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 173,   89],\n",
       "       [  81, 1054]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = best_model.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a93135b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.66      0.67       262\n",
      "           1       0.92      0.93      0.93      1135\n",
      "\n",
      "    accuracy                           0.88      1397\n",
      "   macro avg       0.80      0.79      0.80      1397\n",
      "weighted avg       0.88      0.88      0.88      1397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "76770e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "561a3c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = LinearSVC(random_state=0, tol=1e-5, verbose=1, max_iter=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0eb4754b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(max_iter=50000, random_state=0, tol=1e-05, verbose=1)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ab409054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 127,  135],\n",
       "       [  35, 1100]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = svm_clf.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "77cde40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.48      0.60       262\n",
      "           1       0.89      0.97      0.93      1135\n",
      "\n",
      "    accuracy                           0.88      1397\n",
      "   macro avg       0.84      0.73      0.76      1397\n",
      "weighted avg       0.87      0.88      0.87      1397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9c1da7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svm_clf = svm.SVC(kernel='linear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8f5b2df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8f0c26a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 132,  130],\n",
       "       [  32, 1103]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = svm_clf.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d9fe0f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.50      0.62       262\n",
      "           1       0.89      0.97      0.93      1135\n",
      "\n",
      "    accuracy                           0.88      1397\n",
      "   macro avg       0.85      0.74      0.78      1397\n",
      "weighted avg       0.88      0.88      0.87      1397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1bf6ba",
   "metadata": {},
   "source": [
    "### Hard Voting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3c86b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4005e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(solver='newton-cg')\n",
    "rnd_clf = RandomForestClassifier()\n",
    "smv_clf = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5ecf6537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(solver='newton-cg')),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', smv_clf)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a2ae80a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.8869005010737294\n",
      "RandomForestClassifier 0.9112383679312813\n",
      "SVC 0.8840372226198998\n",
      "VotingClassifier 0.9226914817465999\n"
     ]
    }
   ],
   "source": [
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d38e81bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 176,   86],\n",
       "       [  22, 1113]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = voting_clf.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ab0f5471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.77       262\n",
      "           1       0.93      0.98      0.95      1135\n",
      "\n",
      "    accuracy                           0.92      1397\n",
      "   macro avg       0.91      0.83      0.86      1397\n",
      "weighted avg       0.92      0.92      0.92      1397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef30bf32",
   "metadata": {},
   "source": [
    "## Bagging and Pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "aed279dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0c3ebe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "        DecisionTreeClassifier(), n_estimators=500,\n",
    "        max_samples=100, bootstrap=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "eea8734f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=100,\n",
       "                  n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ca47b747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 179,   83],\n",
       "       [  61, 1074]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = bag_clf.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3a57e58f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.68      0.71       262\n",
      "           1       0.93      0.95      0.94      1135\n",
      "\n",
      "    accuracy                           0.90      1397\n",
      "   macro avg       0.84      0.81      0.83      1397\n",
      "weighted avg       0.89      0.90      0.90      1397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fa747c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
