{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6bc1fe78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T21:58:11.746418Z",
     "start_time": "2022-01-20T21:58:07.524220Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "import re\n",
    "\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6a22ce40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T21:58:11.756429Z",
     "start_time": "2022-01-20T21:58:11.748403Z"
    }
   },
   "outputs": [],
   "source": [
    "#from spacy import displacy\n",
    "#import deplacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a8bc155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "def get_test_units_list(test_df):\n",
    "    data = [(row['text'], dict(id=row['essay_id'])) for ind, row in test_df.iterrows()]\n",
    "    docs = []\n",
    "    data\n",
    "    for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "        doc._.essay_id = context['id']\n",
    "        docs.append(doc)\n",
    "        \n",
    "    segmented_docs = [segmentation(doc, mode='sentence') for doc in docs]\n",
    "    \n",
    "    # Flatten lists (Dissolve docs boundaries and store all units together in one huge list)\n",
    "    units = list(chain.from_iterable(segmented_docs))\n",
    "    return units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "079d2a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Computer has negative effects to children\n",
       "\n",
       "Nowadays, thanks to the development of technology, computer is now indispensable to life."
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    def get_label(span):\n",
    "        \n",
    "        # Gets ADU vs non-ADU LABEL for the span (intended only for sentences)\n",
    "\n",
    "        # Works if the span is larger or equal to the adu\n",
    "\n",
    "        # TODO:\n",
    "        # DOES NOT WORK IF SPAN IS SMALLER THAN ADU, OR IF ADU IS SPLIT BETWEEN TWO SPANS (NEEDS MORE WORK!!!)\n",
    "        # CLAIM VS PREMISE\n",
    "        essay_id = span.doc._.essay_id\n",
    "\n",
    "        span_start = span[0].idx\n",
    "        span_end = span[-1].idx  + len(span[-1])\n",
    "        start_inds = adus[adus['essay_id'] == essay_id ]['start_ind'].values\n",
    "        end_inds = adus[adus['essay_id'] == essay_id ]['end_ind'].values\n",
    "\n",
    "        # Checks if starting index of span is smaller than ADU and the ending index of the span is larger than the ADU\n",
    "        return ((start_inds >= span_start) & (end_inds <= span_end)).any()\n",
    "\n",
    "    \n",
    "units[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cdaa8e31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T21:58:11.854512Z",
     "start_time": "2022-01-20T21:58:11.757411Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_features = ['num_tokens', 'para_starts']\n",
    "span_features = ['word_emb', 'num_tokens', 'num_verbs', 'num_pos_pronouns', 'num_conj_adv', 'num_punct', 'is_para_start',\n",
    "                 'index_in_doc']\n",
    "token_features =['word_emb']\n",
    "\n",
    "# getters that are not used as features\n",
    "span_utilities = ['prev_unit', 'label']\n",
    "# methods\n",
    "span_methods = ['get_nth_unit', 'get_prev_unit_attr']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "extensions_dict = dict(doc_features=doc_features, span_features=span_features+span_utilities,\n",
    "                       token_features=token_features, span_methods=span_methods)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_extensions(extensions_dict=None, force=True):\n",
    "    \n",
    "    # Features that take 'unit' as input refer to the segmentation, they do not work with just any span.\n",
    "    \n",
    "    # Property attributes\n",
    "    \n",
    "    # Store starting and ending indices of spans in the whole doc\n",
    "    # 1 list per each document: [(s1_start, s1_end), (s2_start, s2_end),.., (sn_start, sn_end)]\n",
    "    Doc.set_extension(\"units_index_list\", default=[],force=True)\n",
    "    \n",
    "    # Store essay_id within doc\n",
    "    Doc.set_extension(\"essay_id\", default=None, force=True)\n",
    "\n",
    "    \n",
    "    # Feature Getters\n",
    "    \n",
    "    def get_label(span):\n",
    "        \n",
    "        # Gets ADU vs non-ADU LABEL for the span (intended only for sentences)\n",
    "\n",
    "        # Works if the span is larger or equal to the adu\n",
    "\n",
    "        # TODO:\n",
    "        # DOES NOT WORK IF SPAN IS SMALLER THAN ADU, OR IF ADU IS SPLIT BETWEEN TWO SPANS (NEEDS MORE WORK!!!)\n",
    "        # CLAIM VS PREMISE\n",
    "        essay_id = span.doc._.essay_id\n",
    "\n",
    "        span_start = span[0].idx\n",
    "        span_end = span[-1].idx  + len(span[-1])\n",
    "        start_inds = adus[adus['essay_id'] == essay_id ]['start_ind'].values\n",
    "        end_inds = adus[adus['essay_id'] == essay_id ]['end_ind'].values\n",
    "\n",
    "        # Checks if starting index of span is smaller than ADU and the ending index of the span is larger than the ADU\n",
    "        return ((start_inds >= span_start) & (end_inds <= span_end)).any()\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def get_para_starts(doc):\n",
    "        # Units starting with \\n or preceding \\n are considered as paragraph starts\n",
    "        # if start is 0, start -1 goes back to the last token of the doc\n",
    "\n",
    "        # TODO\n",
    "        # para_ends can be obtained by shifing this list to the right by one position\n",
    "        return [int(doc[start].text =='\\n' or doc[start-1].text=='\\n') for start, end in doc._.units_index_list]\n",
    "    \n",
    "    def get_is_para_start(unit):\n",
    "        \n",
    "        para_starts = unit.doc._.para_starts\n",
    "        unit_ind = unit._.index_in_doc\n",
    "        \n",
    "        return para_starts[unit_ind]\n",
    "        \n",
    "    \n",
    "    def get_word_emb(obj):\n",
    "        return obj.vector\n",
    "    \n",
    "    def get_num_tokens(obj):\n",
    "        return len(obj)\n",
    "    \n",
    "    def get_num_verbs(span):\n",
    "        return sum([1 for token in span if token.pos_ == \"VERB\"])\n",
    "\n",
    "    def get_num_pos_pronouns(span):\n",
    "        return sum([1 for token in span if token.tag_ == \"PRP$\"])\n",
    "\n",
    "    def get_num_pron(span):\n",
    "        return sum([1 for token in span if token.pos_ == \"PRON\"])\n",
    "    \n",
    "    def get_num_conj_adv(span):\n",
    "        conj_advs = ['moreover', 'incidentally', 'next', 'yet', 'finally', 'then', 'for example', 'thus', 'accordingly', 'namely', 'meanwhile', 'that is', 'also', 'undoubtedly', 'all in all', 'lately', 'hence', 'still', 'therefore', 'in addition', 'indeed', 'again', 'so', 'nevertheless', 'besides', 'instead', 'for instance', 'certainly', 'however', 'anyway', 'further', 'furthermore', 'similarly', 'now', 'in conclusion', 'nonetheless', 'thereafter', 'likewise', 'otherwise', 'consequently']\n",
    "        return sum([len(re.findall(adv, span.text.lower())) for adv in conj_advs])\n",
    "    \n",
    "    def get_num_punct(span):\n",
    "        return sum([1 for token in span if token.tag_ == \".\"])\n",
    "    \n",
    "\n",
    "    def get_index_in_doc(span):\n",
    "        \"\"\"Gets index of the segmented unit in the doc\"\"\"\n",
    "        span_start = span.start\n",
    "\n",
    "        # span end not used yet\n",
    "        span_end = span.end\n",
    "\n",
    "        # finds where span_start is in units_index_list [(s1_start, s1_end), (s2_start, s2_end),.., (sn_start, sn_end)]\n",
    "        # returns the index of the corresponding span\n",
    "        return np.where([span.start in range(start, end) for start, end in span.doc._.units_index_list])[0][-1]\n",
    "\n",
    "\n",
    "    def get_prev_unit(span):\n",
    "\n",
    "        return span._.get_nth_unit(span._.index_in_doc-1)\n",
    "    \n",
    "        \n",
    "    def get_nth_unit(span, n):\n",
    "\n",
    "        # Tuple containing the start and end index of the nth span\n",
    "        span_index = span.doc._.units_index_list[n]\n",
    "\n",
    "        # Return nth span\n",
    "        return span.doc[span_index[0]: span_index[1]]\n",
    "\n",
    "\n",
    "\n",
    "    def get_prev_unit_attr(span, attribute):\n",
    "\n",
    "        return span._.prev_unit._.get(attribute)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Iterate list of features and Set Extensions (Just to not manually set extensions one by one)\n",
    "    \n",
    "    for feature in extensions_dict['doc_features']:\n",
    "        Doc.set_extension(feature, force=force, getter=locals()[f\"get_{feature}\"])\n",
    "        \n",
    "    for feature in extensions_dict['span_features']:\n",
    "        Span.set_extension(feature, force=force, getter=locals()[f\"get_{feature}\"])\n",
    "        \n",
    "    for feature in extensions_dict['token_features']:\n",
    "        Token.set_extension(feature, force=force, getter=locals()[f\"get_{feature}\"])\n",
    "        \n",
    "    for method in extensions_dict['span_methods']:\n",
    "        Span.set_extension(method, force=force, method=locals()[method])\n",
    "\n",
    "\n",
    "def segmentation(doc=None ,mode = 'sentence'):\n",
    "    if mode=='paragraph':\n",
    "        pass\n",
    "    if mode=='sentence':\n",
    "        # segment by sentences\n",
    "        units = [sent for sent in doc.sents  if not (sent.text.isspace() or sent.text =='')] \n",
    "        \n",
    "        # keep track of (start, end) of units in doc object\n",
    "        doc._.units_index_list = [(unit.start, unit.end) for unit in units]\n",
    "        return units\n",
    "    \n",
    "    if mode =='avg_n_grams':\n",
    "        # Code to segment with 15 grams here (average)    \n",
    "        pass\n",
    "    if mode=='clause':\n",
    "        # Code to segment by clause\n",
    "        pass\n",
    "    if mode=='token':\n",
    "        return [token for token in doc if not (token.text.isspace() or token.text =='')]\n",
    "\n",
    "def unit2fv(unit, feature_list):\n",
    "    \n",
    "    fv = np.array([unit._.get(feature) for feature in feature_list], dtype='object')\n",
    "    \n",
    "    _fv = np.array([np.reshape(feature, -1) for feature in fv], dtype='object')\n",
    "    \n",
    "    return np.concatenate(_fv)\n",
    "\n",
    "# Run\n",
    "create_extensions(extensions_dict)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c1b48dc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T21:58:11.860513Z",
     "start_time": "2022-01-20T21:58:11.856510Z"
    }
   },
   "outputs": [],
   "source": [
    "# Optional, not used yet. Trying to solve problem that title gets included with the first sentence\n",
    "def add_full_stops(text):\n",
    "    \"\"\"adds full stops to texts that end with \\n missing full stops\"\"\"\n",
    "    return re.sub(\"\\n+(?!\\.)\",'.\\n', text)\n",
    "# Not used\n",
    "def text2doc(text):\n",
    "    # need to use nlp.pipe here instead\n",
    "    return nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ae630742",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T21:58:11.904513Z",
     "start_time": "2022-01-20T21:58:11.861511Z"
    }
   },
   "outputs": [],
   "source": [
    "# INPUTS \n",
    "essays = pd.read_csv(\"../data/output_csv/essays.csv\")\n",
    "adus = pd.read_csv(\"../data/output_csv/adus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9620bdb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T21:58:11.907514Z",
     "start_time": "2022-01-20T21:58:11.905515Z"
    }
   },
   "outputs": [],
   "source": [
    "####### TEST\n",
    "# in_text = essays.iloc[23].text\n",
    "# doc = nlp(in_text)\n",
    "# units=segmentation(doc)\n",
    "# doc._.para_starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dab688e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADU_index</th>\n",
       "      <th>ADU</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>label</th>\n",
       "      <th>start_ind</th>\n",
       "      <th>end_ind</th>\n",
       "      <th>claim_type</th>\n",
       "      <th>ADU_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1</td>\n",
       "      <td>MajorClaim 503 575\\twe should attach more impo...</td>\n",
       "      <td>essay001</td>\n",
       "      <td>train</td>\n",
       "      <td>503</td>\n",
       "      <td>575</td>\n",
       "      <td>MajorClaim</td>\n",
       "      <td>we should attach more importance to cooperatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T2</td>\n",
       "      <td>MajorClaim 2154 2231\\ta more cooperative attit...</td>\n",
       "      <td>essay001</td>\n",
       "      <td>train</td>\n",
       "      <td>2154</td>\n",
       "      <td>2231</td>\n",
       "      <td>MajorClaim</td>\n",
       "      <td>a more cooperative attitudes towards life is m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T3</td>\n",
       "      <td>Claim 591 714\\tthrough cooperation, children c...</td>\n",
       "      <td>essay001</td>\n",
       "      <td>train</td>\n",
       "      <td>591</td>\n",
       "      <td>714</td>\n",
       "      <td>Claim</td>\n",
       "      <td>through cooperation, children can learn about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T4</td>\n",
       "      <td>Premise 716 851\\tWhat we acquired from team wo...</td>\n",
       "      <td>essay001</td>\n",
       "      <td>train</td>\n",
       "      <td>716</td>\n",
       "      <td>851</td>\n",
       "      <td>Premise</td>\n",
       "      <td>What we acquired from team work is not only ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T5</td>\n",
       "      <td>Premise 853 1086\\tDuring the process of cooper...</td>\n",
       "      <td>essay001</td>\n",
       "      <td>train</td>\n",
       "      <td>853</td>\n",
       "      <td>1086</td>\n",
       "      <td>Premise</td>\n",
       "      <td>During the process of cooperation, children ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6084</th>\n",
       "      <td>T11</td>\n",
       "      <td>Premise 1275 1339\\tindirectly they will learn ...</td>\n",
       "      <td>essay402</td>\n",
       "      <td>train</td>\n",
       "      <td>1275</td>\n",
       "      <td>1339</td>\n",
       "      <td>Premise</td>\n",
       "      <td>indirectly they will learn how to socialize ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6085</th>\n",
       "      <td>T12</td>\n",
       "      <td>Premise 1341 1388\\tThat will make children get...</td>\n",
       "      <td>essay402</td>\n",
       "      <td>train</td>\n",
       "      <td>1341</td>\n",
       "      <td>1388</td>\n",
       "      <td>Premise</td>\n",
       "      <td>That will make children getting lots of friends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6086</th>\n",
       "      <td>T13</td>\n",
       "      <td>Premise 1393 1436\\tthey can contribute positiv...</td>\n",
       "      <td>essay402</td>\n",
       "      <td>train</td>\n",
       "      <td>1393</td>\n",
       "      <td>1436</td>\n",
       "      <td>Premise</td>\n",
       "      <td>they can contribute positively to community</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6087</th>\n",
       "      <td>T14</td>\n",
       "      <td>Premise 1448 1525\\tplaying sport makes childre...</td>\n",
       "      <td>essay402</td>\n",
       "      <td>train</td>\n",
       "      <td>1448</td>\n",
       "      <td>1525</td>\n",
       "      <td>Premise</td>\n",
       "      <td>playing sport makes children getting healthy a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6088</th>\n",
       "      <td>T15</td>\n",
       "      <td>Claim 916 965\\tplaying sports will give good e...</td>\n",
       "      <td>essay402</td>\n",
       "      <td>train</td>\n",
       "      <td>916</td>\n",
       "      <td>965</td>\n",
       "      <td>Claim</td>\n",
       "      <td>playing sports will give good effects on children</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6089 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ADU_index                                                ADU  essay_id  \\\n",
       "0           T1  MajorClaim 503 575\\twe should attach more impo...  essay001   \n",
       "1           T2  MajorClaim 2154 2231\\ta more cooperative attit...  essay001   \n",
       "2           T3  Claim 591 714\\tthrough cooperation, children c...  essay001   \n",
       "3           T4  Premise 716 851\\tWhat we acquired from team wo...  essay001   \n",
       "4           T5  Premise 853 1086\\tDuring the process of cooper...  essay001   \n",
       "...        ...                                                ...       ...   \n",
       "6084       T11  Premise 1275 1339\\tindirectly they will learn ...  essay402   \n",
       "6085       T12  Premise 1341 1388\\tThat will make children get...  essay402   \n",
       "6086       T13  Premise 1393 1436\\tthey can contribute positiv...  essay402   \n",
       "6087       T14  Premise 1448 1525\\tplaying sport makes childre...  essay402   \n",
       "6088       T15  Claim 916 965\\tplaying sports will give good e...  essay402   \n",
       "\n",
       "      label  start_ind  end_ind  claim_type  \\\n",
       "0     train        503      575  MajorClaim   \n",
       "1     train       2154     2231  MajorClaim   \n",
       "2     train        591      714       Claim   \n",
       "3     train        716      851     Premise   \n",
       "4     train        853     1086     Premise   \n",
       "...     ...        ...      ...         ...   \n",
       "6084  train       1275     1339     Premise   \n",
       "6085  train       1341     1388     Premise   \n",
       "6086  train       1393     1436     Premise   \n",
       "6087  train       1448     1525     Premise   \n",
       "6088  train        916      965       Claim   \n",
       "\n",
       "                                               ADU_text  \n",
       "0     we should attach more importance to cooperatio...  \n",
       "1     a more cooperative attitudes towards life is m...  \n",
       "2     through cooperation, children can learn about ...  \n",
       "3     What we acquired from team work is not only ho...  \n",
       "4     During the process of cooperation, children ca...  \n",
       "...                                                 ...  \n",
       "6084  indirectly they will learn how to socialize ea...  \n",
       "6085    That will make children getting lots of friends  \n",
       "6086        they can contribute positively to community  \n",
       "6087  playing sport makes children getting healthy a...  \n",
       "6088  playing sports will give good effects on children  \n",
       "\n",
       "[6089 rows x 8 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "929accd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T21:58:11.915515Z",
     "start_time": "2022-01-20T21:58:11.908514Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pipelinev1\n",
    "\n",
    "def text2fv(df):\n",
    "    data = [(row['text'], dict(id=row['essay_id'])) for ind, row in df.iterrows()]\n",
    "    docs = []\n",
    "    data\n",
    "    for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "        doc._.essay_id = context['id']\n",
    "        docs.append(doc)\n",
    "        \n",
    "    segmented_docs = [segmentation(doc, mode='sentence') for doc in docs]\n",
    "    \n",
    "    # Flatten lists (Dissolve docs boundaries and store all units together in one huge list)\n",
    "    units = list(chain.from_iterable(segmented_docs))\n",
    "    \n",
    "    X_features = span_features\n",
    "    \n",
    "\n",
    "    X = np.array([unit2fv(unit, X_features) for unit in units])\n",
    "    y = np.array([int(unit._.label) for unit in units])\n",
    "    \n",
    "    return X,y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0133b31f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T21:58:16.461653Z",
     "start_time": "2022-01-20T21:58:11.916516Z"
    }
   },
   "outputs": [],
   "source": [
    "train = essays[essays['label'] =='train']\n",
    "test =essays[essays['label'] =='test']\n",
    "\n",
    "X_train, y_train = text2fv(train)\n",
    "\n",
    "X_test, y_test = text2fv(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "7f8676ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "\n",
    "essay24adus = adus[adus['essay_id'] == 'essay024']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "2b53a180",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_list = doc._.units_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "3b13908e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 23),\n",
       " (23, 46),\n",
       " (46, 56),\n",
       " (56, 70),\n",
       " (70, 98),\n",
       " (98, 124),\n",
       " (124, 149),\n",
       " (149, 186),\n",
       " (186, 209),\n",
       " (209, 238),\n",
       " (238, 269),\n",
       " (269, 270),\n",
       " (270, 298),\n",
       " (298, 329),\n",
       " (329, 330)]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ef4d71f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "."
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "b8ff47c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_unit = [doc[start].idx for start, end in ind_list]\n",
    "end_unit = [ doc[end-1].idx+1 for start, end in ind_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "80279476",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_end = essay24adus[['start_ind', 'end_ind']]\n",
    "\n",
    "for i, row in start_end.iterrows():\n",
    "    span_s = np.where(np.array(start_unit) <row['start_ind'])[0][-1]\n",
    "    span_e = np.where(np.array(end_unit) <row['end_ind'])[0][-1]\n",
    "    \n",
    "    print(row['start_ind'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "52113c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "5b6d0c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 132),\n",
       " (133, 260),\n",
       " (261, 303),\n",
       " (303, 374),\n",
       " (375, 522),\n",
       " (522, 664),\n",
       " (665, 796),\n",
       " (797, 974),\n",
       " (974, 1094),\n",
       " (1095, 1240),\n",
       " (1241, 1391),\n",
       " (1391, 1392),\n",
       " (1392, 1531),\n",
       " (1532, 1704),\n",
       " (1704, 1705)]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.array(start_unit) <1480)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "4157716a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.array(start_unit) <1480)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "07347e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1392"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_unit[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "93f6e141",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[132,\n",
       " 260,\n",
       " 303,\n",
       " 374,\n",
       " 522,\n",
       " 664,\n",
       " 796,\n",
       " 974,\n",
       " 1094,\n",
       " 1240,\n",
       " 1391,\n",
       " 1392,\n",
       " 1531,\n",
       " 1704,\n",
       " 1705]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_unit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c79ef4",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9f8a3444",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T21:58:16.774676Z",
     "start_time": "2022-01-20T21:58:16.462655Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4e43648c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T21:58:16.819681Z",
     "start_time": "2022-01-20T21:58:16.777676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='newton-cg')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver='newton-cg')\n",
    "logreg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f900ce6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T21:58:16.833682Z",
     "start_time": "2022-01-20T21:58:16.820682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.45      0.57       250\n",
      "           1       0.89      0.97      0.93      1134\n",
      "\n",
      "    accuracy                           0.88      1384\n",
      "   macro avg       0.83      0.71      0.75      1384\n",
      "weighted avg       0.87      0.88      0.86      1384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds_lr = logreg.predict(X_test)\n",
    "print(classification_report(y_test, preds_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5ceb493d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T21:58:16.895685Z",
     "start_time": "2022-01-20T21:58:16.835682Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0f59ec3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T21:58:17.050698Z",
     "start_time": "2022-01-20T21:58:16.896688Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1d9fdbcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.33700159e-03,  5.74849825e-03, -8.07924047e-02, ...,\n",
       "         1.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [-1.06555268e-01,  2.89857566e-01, -5.34156524e-02, ...,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00],\n",
       "       [-1.15756266e-01,  2.25932971e-01, -5.61617054e-02, ...,\n",
       "         1.00000000e+00,  0.00000000e+00,  2.00000000e+00],\n",
       "       ...,\n",
       "       [-4.53010015e-02,  2.28496000e-01, -2.38107115e-01, ...,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.40000000e+01],\n",
       "       [-7.72796273e-02,  2.66258746e-01, -1.75650015e-01, ...,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.50000000e+01],\n",
       "       [-2.38838326e-02,  2.44683713e-01, -1.54835701e-01, ...,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.60000000e+01]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6cccf1d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T21:58:17.074700Z",
     "start_time": "2022-01-20T21:58:17.052702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 145,  105],\n",
       "       [  19, 1115]], dtype=int64)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = rf.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dba38ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4b778181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.30584666e-02,  1.70649409e-01,  1.19616408e-02, ...,\n",
       "         1.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 4.34298851e-02,  9.00983885e-02, -8.72147977e-02, ...,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00],\n",
       "       [ 1.32289603e-01,  7.90781230e-02, -1.53517351e-01, ...,\n",
       "         1.00000000e+00,  1.00000000e+00,  2.00000000e+00],\n",
       "       ...,\n",
       "       [-5.44680282e-02,  8.68857130e-02, -1.39478177e-01, ...,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+01],\n",
       "       [-2.51547489e-02,  1.78158998e-01, -4.51728217e-02, ...,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.10000000e+01],\n",
       "       [-2.11575646e-02,  1.66456774e-01, -1.57130525e-01, ...,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.20000000e+01]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4a4c81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.64      0.74       262\n",
      "           1       0.92      0.98      0.95      1135\n",
      "\n",
      "    accuracy                           0.92      1397\n",
      "   macro avg       0.90      0.81      0.85      1397\n",
      "weighted avg       0.92      0.92      0.91      1397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8af112",
   "metadata": {},
   "source": [
    "## CrossValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f7b1433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84a52d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(random_state=42)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Stochastic Gradient Descent (SGD) classifier, \n",
    "This classifier has the advantage of being capable of handling very large datasets efficiently\"\"\"\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b2df2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7224231464737794\n",
      "[[181  53]\n",
      " [254 618]]\n",
      "0.8707052441229657\n",
      "[[145  89]\n",
      " [ 54 818]]\n",
      "0.8471971066907775\n",
      "[[110 124]\n",
      " [ 45 827]]\n",
      "0.8426763110307414\n",
      "[[ 69 164]\n",
      " [ 10 863]]\n",
      "0.8090497737556561\n",
      "[[183  50]\n",
      " [161 711]]\n"
     ]
    }
   ],
   "source": [
    "skfolds = StratifiedKFold(n_splits=5, random_state=42)\n",
    "\n",
    "best_model = None \n",
    "precision = 0\n",
    "for train_index, test_index in skfolds.split(X_train, y_train):\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    X_train_folds = X_train[train_index]\n",
    "    y_train_folds = y_train[train_index]\n",
    "    X_test_fold = X_train[test_index]\n",
    "    y_test_fold = y_train[test_index]\n",
    "    \n",
    "    clone_clf.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    if precision < n_correct / len(y_pred):\n",
    "        best_model = clone_clf\n",
    "        precision = n_correct / len(y_pred)\n",
    "    print(n_correct / len(y_pred))\n",
    "    print(confusion_matrix(y_test_fold, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "feda26fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 173,   89],\n",
       "       [  81, 1054]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = best_model.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ad386a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.66      0.67       262\n",
      "           1       0.92      0.93      0.93      1135\n",
      "\n",
      "    accuracy                           0.88      1397\n",
      "   macro avg       0.80      0.79      0.80      1397\n",
      "weighted avg       0.88      0.88      0.88      1397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dea33470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d6595ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = LinearSVC(random_state=0, tol=1e-5, verbose=1, max_iter=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cf40b2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(max_iter=50000, random_state=0, tol=1e-05, verbose=1)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "38e132d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 127,  135],\n",
       "       [  35, 1100]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = svm_clf.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5b04d52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.48      0.60       262\n",
      "           1       0.89      0.97      0.93      1135\n",
      "\n",
      "    accuracy                           0.88      1397\n",
      "   macro avg       0.84      0.73      0.76      1397\n",
      "weighted avg       0.87      0.88      0.87      1397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cd95a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svm_clf = svm.SVC(kernel='linear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9d9d5329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e8a4d7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 132,  130],\n",
       "       [  32, 1103]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = svm_clf.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "21ab12a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.50      0.62       262\n",
      "           1       0.89      0.97      0.93      1135\n",
      "\n",
      "    accuracy                           0.88      1397\n",
      "   macro avg       0.85      0.74      0.78      1397\n",
      "weighted avg       0.88      0.88      0.87      1397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acea33d",
   "metadata": {},
   "source": [
    "### Hard Voting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ef3e731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "abe7184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(solver='newton-cg')\n",
    "rnd_clf = RandomForestClassifier()\n",
    "smv_clf = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9078be1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(solver='newton-cg')),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', smv_clf)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "63a70640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.8869005010737294\n",
      "RandomForestClassifier 0.9112383679312813\n",
      "SVC 0.8840372226198998\n",
      "VotingClassifier 0.9226914817465999\n"
     ]
    }
   ],
   "source": [
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "99a1a438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 176,   86],\n",
       "       [  22, 1113]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = voting_clf.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "30072fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.77       262\n",
      "           1       0.93      0.98      0.95      1135\n",
      "\n",
      "    accuracy                           0.92      1397\n",
      "   macro avg       0.91      0.83      0.86      1397\n",
      "weighted avg       0.92      0.92      0.92      1397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb739c6a",
   "metadata": {},
   "source": [
    "## Bagging and Pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cd3c1dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "35bf5af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "        DecisionTreeClassifier(), n_estimators=500,\n",
    "        max_samples=100, bootstrap=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "846badca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=100,\n",
       "                  n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b67cfbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 179,   83],\n",
       "       [  61, 1074]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = bag_clf.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "80703773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.68      0.71       262\n",
      "           1       0.93      0.95      0.94      1135\n",
      "\n",
      "    accuracy                           0.90      1397\n",
      "   macro avg       0.84      0.81      0.83      1397\n",
      "weighted avg       0.89      0.90      0.90      1397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd810ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7814b21a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb5ccf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae034efe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
